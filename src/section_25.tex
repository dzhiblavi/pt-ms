\section{Цепи Маркова. Марковская зависимость. Переходные вероятности.
Предельные вероятности. Схемы блужданий.}

\subsection{Марковская зависимость}

Пусть $G$ -- эксперимент с конечным числом исходов $E_i$. Будем неограниченно
повторять эксперимент $G$. Рассмотрим связанную с этим процессом
последовательность случайных величин $X_i,~ i \in \mathbb{N}_0$. Будем для
краткости обозначать $E_i = i$ там, где это удобно.

\begin{definition}
    Последовательность случайных величин $X_i$ образует \textit{цепь Маркова}, 
    если выполнено
    \[
        P(X_l = j \mid X_0 = k_0, \ldots, X_{l - 1} = i) =
        P(X_l = j \mid X_{l - 1} = i) \defeq p^{(l)}_{i, j}
    .\]
\end{definition}

\begin{definition}
    \textit{Начальное состояние} цепи Маркова задается распределением
    $X_0$:
    \[
        P(X_0 = i) = p_i(0)
    .\]
\end{definition}

\begin{remark}
    Отличие Марковской цепи от других дискретных процессов состоит в том, что
    результат очередного эксперимента зависит исключительно от результата
    предыдущего эксперимента.
\end{remark}

\begin{definition}
     Вероятности перейти из одного состояния в другое за один шаг можно
     представить в виде \textit{матрицы переходов}
     \[
         P = \begin{pmatrix}
            p_{1, 1} & p_{1, 2} & \ldots & p_{1, n} \\
            p_{2, 1} & p_{2, 2} & \ldots & p_{2, n} \\
            \vdots & \vdots & \ddots & \vdots \\
            p_{n, 1} & p_{n, 2} & \ldots & p_{n, n}
         \end{pmatrix}   
     .\]
\end{definition}

\begin{remark}
    Отметим пару очевидных свойств матрицы переходов:
    \begin{itemize}
        \item $p_{i, j} \geqslant 0$.
        \item $\forall i~ \sum_{j = 1}^n{p_{i, j}} = 1$.
    \end{itemize}
\end{remark}

\begin{definition}
    Матрица, удовлетворяющая вышеперечисленным свойствам, называется
    \textit{стохастической}.
\end{definition}

\begin{remark}
    Понятно, что любая матрица переходов является стохастической. Кроме того,
    любая стохастичекая матрица является матрицей переходов соответствующей
    Марковской цепи.
\end{remark}

\begin{definition}
    Обозначим $p_{i, j}(k)$ -- вероятность перехода из состояния $i$ в состояние
    $j$ за $k$ шагов:
    \[
        p_{i, j}(k) = P(X_{k + l} = j \mid X_l = i) = P(X_k = j \mid X_0 = i)
    .\]
    Матрицу из таких вероятностей обозначим $P_k$.
\end{definition}

\begin{theorem}
    $P_k = P^k$.
\end{theorem}
\begin{proof}
    По формуле полной вероятности при $k > 1$ имеем
    \[
        p_{i, j}(k + 1) = \sum_{l = 1}^n{P(X_k = l \mid X_0 = i) p_{l, j}}
        = \sum_{l = 1}^n{p_{i, l}(k) p_{l, j}}
    .\]
    В матричном виде
    \[
        P_{k + 1} = P_{k} \cdot P
    .\]
    Откуда по индукции с базой $P_1 = P$ получаем требуемое.
\end{proof}

\begin{remark}
    Если задано начальное распределение $p_i(0) = P(X_0 = i)$, то аналогичным
    образом можно получить распределение вероятностей для произвольного момента
    $t$:
    \[
        p_i(t) = P(X_t = i) = \sum_{l = 1}^n{p_l(0) p_{l, i}(t)}
    .\]
\end{remark}

\subsection{Предельные вероятности}

\begin{definition}
    Цепь Маркова, для которой для всех $j$ существует предел
    \[
        p_j = \lim_{t \to +\infty}{p_{i, j}(t)}
    .\]
    называется \textit{эргодической}.
\end{definition}

\begin{theorem}
    Если цепь Маркова является эргодической, то система уравнений
    \[
        x_j = \sum_{l = 1}^n{x_l p_{l, j}},~~ \sum_{l = 1}^n{x_l} = 1
    \]
    имеет единственное решение
    \[
        x_j = p_j = \lim_{t \to +\infty}{p_{i, j}(t)}
    .\]
\end{theorem}
\begin{proof}
    \enewline
    \begin{itemize}
        \item Покажем, что $x_j = p_j$ действительно решение системы. Для 
            этого выпишем равенства из доказательства теоремы $P_k = P^k$:
            \[
                p_{i, j}(k + 1) = \sum_{l = 1}^n{p_{i, l}(k) p_{l, j}}
            .\]
            И перейдем в нём к предельному переходу при $k \to +\infty$:
            \[
                p_j = \sum_{l = 1}^n{p_l p_{l, j}}  
            .\]
            Для проверки последнего уравнения системы перейдем к пределу
            при $k \to +\infty$ в уравнении
            \[
                \sum_{j = 1}^n{p_{i, j}(k)} = 1
            .\]
        \item Проверим теперь, что это решение единственное, то есть если
            $x_l$ -- решение системы, то $x_l = p_l$. Используя индукцию
            и первые уравнения системы легко показать, что
            \[
                \forall m \in \bN~ x_j = \sum_{k = 1}^n{x_k p_{k, j}(m)}
            .\]
            Действительно, при $m = 1$ имеем $p_{l, j}(1) = p_{l, j}$. 
            Предположим, что утверждение справедливо при $m = l$, подставим
            вместо $x_k$ выражение из первого уравнения системы имеем
            \[
                x_j = \sum_{k = 1}^n{\sum_{i = 1}^n{x_i p_{i, k} p_{k, j}(l)}} =
                \sum_{i = 1}^n{x_i p_{i, j}(l + 1)}
            .\]
            Осталось перейти к пределу при $m \to +\infty$:
            \[
                x_j = \sum_{k = 1}^n{x_k p_j} = p_j \cdot \sum_{k = 1}^n{x_k} 
                = p_j \cdot 1 = p_j
            .\] 
    \end{itemize}
\end{proof}

\begin{theorem}
    Если все элементы матрицы $P$ строго положительны, то соответствующая
    цепь маркова является эргодической.
\end{theorem}
\begin{proof}
    Обозначим
    \[
        M_j(t) = \max_{1 \leqslant i \leqslant n}{p_{i, j}(t)},~~
        m_j(t) = \min_{1 \leqslant i \leqslant n}{p_{i, j}(t)}
    .\]
    Выполнено:
    \[
        p_{i, j}(t + 1) = \sum_{k = 1}^n{p_{i, k} p_{k, j}(t)},~~
        m_j(t) \leqslant p_{k, j}(t) \leqslant M_j(t)
    .\]
    Следовательно, для любого $i$ имеем:
    \[
        m_j(t) = m_j(t) \sum_{k = 1}^n{p_{i, j}} \leqslant p_{i, j}(t + 1)
        \leqslant M_j(t) \sum_{k = 1}^n{p_{i, j}} = M_j(t)
    .\]
    Обозначим за $k$, $l$ индексы, при которых для фиксированного $j$ достигается
    $M_j(t + 1)$ и $m_j(t + 1)$ соответственно:
    \[
        p_{k, j}(t + 1) = M_j(t + 1),~~
        p_{l, j}(t + 1) = m_j(t + 1)
    .\]
    Пользуясь предыдущим неравенством, получаем:
    \[
        m_j(t) \leqslant m_j(t + 1) \leqslant M_j(t + 1) \leqslant M_j(t)
    .\]
    Поскольку последовательности $m_j(t)$ и $M_j(t)$ ограничены и монотонны,
    существуют пределы:
    \[
        \lim_{t \to +\infty}{M_j(t)} = M_j,~~
        \lim_{t \to +\infty}{m_j(t)} = m_j
    .\]
    Для доказательства теоремы осталось показать, что 
    \[
        \lim_{t \to +\infty}{M_j(t) - m_j(t)} = 0
    .\]
    Действительно, тогда $p_{i, j}(t)$ окажется зажатой между 
    двумя последовательностями, сходящимися к одному числу, то есть будет иметь
    предел. Имеем
    \begin{align*}
        M_j(t + 1) - m_j(t + 1) 
        &= p_{k, j}(t + 1) - p_{l, j}(t + 1) = \sum_{m = 1}^n{p_{k, m} 
        p_{m, j}(t)} - \sum_{m = 1}^n{p_{l, m} p_{m, j}(t)} \\
        &= \sum_{m = 1}^n{(p_{k, m} - p_{l, m}) p_{m, j}(t)} \\
        &= {\sum_{m = 1}^n}^+{(p_{k, m} - p_{l, m}) p_{m, j}(t) } +
        {\sum_{m = 1}^n}^-{(p_{k, m} - p_{l, m}) p_{m, j}(t)}
    .\end{align*}
    $\sum^+$ -- сумма положительных слагаемых, $\sum^-$ -- сумма отрицательных.
    Верно неравенство
    \[
        m_j(t) \leqslant p_{m, j}(t) \leqslant M_j(t)
    .\]
    Поэтому:
    \[
        M_j(t + 1) - m_j(t + 1) \leqslant M_j(t) {\sum_{m = 1}^n}^+{(p_{k, m} - p_{l, m})}
        + m_j(t) {\sum_{m = 1}^n}^-{(p_{k, m} - p_{l, m})}
    .\]
    Поскольку
    \[
        0 = 1 - 1 = \sum_{m = 1}^n{(p_{k, m} - p_{l, m})} = 
        {\sum_{m = 1}^n}^+{(p_{k, m} - p_{l, m})} 
        + {\sum_{m = 1}^n}^-{(p_{k, m} - p_{l, m})},~~ \min{p_{i, j}} > 0
    \]
    имеем
    \[
        d_{l, k} = {\sum_{m = 1}^n}^+{(p_{k, m} - p_{l, m})} 
        = {\sum_{m = 1}^n}^-{(p_{k, m} - p_{l, m})} = d < 1
    .\]
    Тогда получаем
    \[
        M_j(t + 1) - m_j(t + 1) = d \cdot (M_j(t) - m_j(t))
    .\]
    Отсюда имеем
    \[
        M_j(t) - m_j(t) \leqslant d^t \xrightarrow[n \to +\infty]{} 0
    .\]
\end{proof}

\begin{corollary}
    Пусть при некотором $N$ все элементы матрицы $P^N$ строго положительны.
    Тогда соответствующая цепь Маркова является эргодической.
\end{corollary}
\begin{proof}
    Рассмотрим Марковскую цепь с $Q = P^N$. Тогда по предыдущей теореме верно
    \[
        \exists \lim_{t \to +\infty}{q_{i, j}(t)} = \lim_{t \to +\infty}
        {p_{i, j}(Nt)} = p_j
    .\]
    Вычислим требуемый предел:
    \[
        \lim_{t \to +\infty}{p_{i, j}(t)} = \lim_{t \to +\infty}
        {p_{i, j}(k + Nt)} = \lim_{t \to +\infty}{\parens*{\sum_{l = 1}^n{
        p_{i, l}(k) p_{l, j}(Nt)}}} = p_j \cdot \sum_{l = 1}^n{p_{i, l}(k)} = p_j
    .\]
\end{proof}

\subsection{Схемы блужданий}
TODO

