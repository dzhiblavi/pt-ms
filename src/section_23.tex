\section{Общая линейная модель или задачи регрессии}
%:: a:dzhiblavi

В приложениях часто возникают задачи о наблюдениях, зависящих от изменяющихся
параметров эксперимента. Пусть проводится $n$ экспериментов с $m$ изменяющимися
параметрами, причем в $i$-м из них набор параметров выглядит следующим образом:
\[
    x_i = \begin{pmatrix}
        x_{i, 1} & x_{i, 2} & \ldots & x_{i, m}
    \end{pmatrix}
.\]
Тогда всем экспериментам сразу соответствует матрица
\[
    X = \begin{pmatrix}
        x_1 \\
        x_2 \\
        \vdots \\
        x_n
    \end{pmatrix} = \begin{pmatrix}
        x_{1, 1} & x_{1, 2} & \ldots & x_{1, m} \\
        x_{2, 1} & x_{2, 2} & \ldots & x_{2, m} \\
        \vdots \\
        x_{n, 1} & x_{n, 2} & \ldots & x_{n, m}
    \end{pmatrix}
.\]
Эта матрица не является набором случайных чисел.

\begin{definition}
    \textit{Задачи регрессии} или \textit{общие линейные модели} соответствуют
    предположению, что наблюдаемые в $i$-м эксперименте данные $y_i$ зависят 
    от параметров линейно с точностью до нормально распределенной с нулевым
    матожиданием ошибки $\xi_i \sim N(0, \sigma^2)$, причем $\xi_i$ независимы:
    \[
        y_i = \sum_{k = 1}^{m}{\theta_k x_{i, k}} + \xi_i
    .\]
    Или в матричном виде сразу для всех $n$ экспериментов:
    \[
        Y = X \theta + \xi
    .\]
    В рамках задачи регрессии требуется по данным $X, Y$ восстановить набор
    коэффициентов $\theta = (\theta_1, \ldots, \theta_m)$.
\end{definition}

\begin{definition}
    В рамках текущей задачи $X$ называется \textit{регрессором}, а
    $Y$ --  \textit{откликом}.
\end{definition}

\begin{definition}
    Можно поставить задачу линейной регрессии по-другому. Пусть имеется
    $m$ функций $\f_k \colon \R \to \R$ и одномерный параметр $z_i$. Тогда
    нужно восстановить зависимость такого вида:
    \[
        y_i = \sum_{k = 1}^{m}{\theta_k \f_k(z_i)} + \xi_i
    .\]
\end{definition}

Построим решение задачи линейной регрессии со следующими дополнительными
условиями:
\begin{itemize}
    \item $Y_i \sim N(m_i, \sigma^2)$.
    \item Набор параметров $X$ подобран таким образом, чтобы его столбцы
        были линейно независимы.
\end{itemize}

Для оценивания коэффициентов $\theta_i$ применим метод максимального
правдоподобия. Вычислим функцию правдоподобия:
\[
    L(Y) = \parens*{\frac{1}{\sigma \sqrt{2 \pi}}}^n \exp
    {\parens*{-\frac{1}{2 \sigma^2} \sum_{i = 1}^{n}{(Y_i - m_i)^2}}},~
    m_i = \sum_{k = 1}^{m}{\theta_k x_{i, k}}
.\]
Тогда логарифмическая функция правдоподобия имеет вид
\[
    l(Y) = \ln{L(Y)} = C - \frac{1}{2 \sigma^2} \sum_{i = 1}^{n}
    {\parens*{Y_i - \sum_{k = 1}^m{\theta_k x_{i, k}}}^2}
.\]
Для максимизации функции правдоподобия осталось найти наименее удаленную от $Y$
точку в пространстве $\cL$, натянутом на $x_1, \ldots, x_m$. Обозначим
искомую точку за $\b = X \theta^*$. Тогда необходимым условием минимума является:
\[
    Y - \b ~\bot~ \cL
.\]
Это условие эквивалентно системе
\[
    \forall k~ Y - \b ~\bot~ x_k
.\]
Раскрываем ортогональность:
\[
    \forall k~ \scp{Y - \b}{x_k} = 0
.\]
И переписываем в матричном виде:
\[
    X^{\top} \cdot (Y - X \theta^*) = 0
.\]
Выражаем $\theta^*$:
\[
    \theta^* = (X^{\top} X)^{-1} \cdot X^\top Y
.\]
Матрица $X^\top X$ положительно определена в случае линейной независимости
$x_l$, поэтому решение существует и единственно.

